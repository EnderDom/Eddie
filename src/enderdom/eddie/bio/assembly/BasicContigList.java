package enderdom.eddie.bio.assembly;

import java.io.File;
import java.io.IOException;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.List;

import org.apache.log4j.Logger;

import enderdom.eddie.bio.sequence.BioFileType;
import enderdom.eddie.bio.sequence.Contig;
import enderdom.eddie.bio.sequence.ContigList;
import enderdom.eddie.bio.sequence.UnsupportedTypeException;

public class BasicContigList implements ContigList{

	Contig[] records;
	int iteration =0;
	Logger logger = Logger.getRootLogger();
	
	public BasicContigList(Contig[] list){
		records = list;
	}
	
	public BasicContigList(List<Contig> list){
		this.records = List2RecordArray(list);
	}
	
	public BasicContigList(File f, BioFileType t) throws Exception{
		if(t == BioFileType.ACE){
			parseAce(f);
		}
		else if(t == BioFileType.SAM || t == BioFileType.BAM){
			logger.warn("Warning no accompanying reference/consensus fasta file, SAM data will not have consensus sequence");
			logger.warn("Use reference sequences generated by assembler or use samtools to generate consensus (@see samtools mpileup)");
			this.records = SAMParseWrapper.parseSAM(f, null);
		}
		else throw new UnsupportedTypeException("Biofile type is not ACE or SAM, do not support " + t.toString());
	}
	
	public BasicContigList(File f, File f2, BioFileType t) throws Exception{
		if(t == BioFileType.SAM){
			this.records = SAMParseWrapper.parseSAM(f, f2);
		}
		else throw new UnsupportedTypeException("Biofile type is not ACE or SAM, do not support " + t.toString());
	}
	

	private void parseAce(File f) throws IOException{
		ACEFileParser parser = new ACEFileParser(f);
		if(parser.getContigSize() != 0){
			logger.debug("Parser claims file has " + parser.getContigSize()+ " contigs");
			records = new Contig[parser.getContigSize()];
			int c=0;
			while(parser.hasNext()){
				System.out.print("\rParsing record "+(c+1)+ " of "+ parser.getContigSize()+"    " );
				records[c]=(Contig)parser.next();
				c++;
			}
			System.out.println();
			logger.debug("Parser gave us " + c+ " contigs");
		}
		else{
			LinkedList<Contig> recs = new LinkedList<Contig>();
			while(parser.hasNext())recs.add((Contig)parser.next());
			this.records= List2RecordArray(recs);
		}
	}
	
	public static Contig[] List2RecordArray(List<Contig> list){
		Contig[] records = new Contig[list.size()];
		for(int i =0; i < list.size();i++)records[i]=list.get(i);
		return records;
	}

	public boolean hasNext() {
		return iteration < this.records.length;
	}

	public Contig next() {
		iteration++;
		return records[iteration-1];
	}

	public void remove() {
		Contig[] tmp = new Contig[records.length-1];
		int i =0;
		for(; i < iteration; i++)tmp[i]=records[i];
		for(; i < tmp.length; i++)tmp[i]=records[i+1];
		this.records = tmp;
	}

	public Contig getContig(String name) {
		for(int i=0; i < records.length; i++){
			if(records[i].getConsensus().getIdentifier().contentEquals(name)){
				return records[i];
			}
		}
		return null;
	}

	public Contig getContig(int i) {
		return records[i];
	}

	public String[] getContigNames() {
		String[] names = new String[records.length];
		for(int i =0; i < names.length ;i++)names[i]=records[i].getConsensus().getIdentifier();
		return names;
	}

	public LinkedHashMap<String, String> getConsensusAsMap() {
		LinkedHashMap<String, String> map = new LinkedHashMap<String, String>();
		for(int i =0; i < records.length;i++){
			map.put(records[i].getConsensus().getIdentifier(), records[i].getConsensus().getSequence());
		}
		return map;
	}

	public int size() {
		return records.length;
	}
	
	
}
